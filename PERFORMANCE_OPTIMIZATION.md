# ä»£ç æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

æœ¬æ–‡æ¡£æä¾›äº†é’ˆå¯¹UNetæ–‡å­—æ°´å°æ£€æµ‹é¡¹ç›®çš„å…¨é¢æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆï¼Œä¸»è¦è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š

1. **å†…å­˜ç®¡ç†ä¼˜åŒ–** - å‡å°‘GPUå†…å­˜å ç”¨å’Œå†…å­˜æ³„æ¼
2. **æ‰¹å¤„ç†ä¼˜åŒ–** - æé«˜æ•°æ®åŠ è½½å’Œå¤„ç†æ•ˆç‡
3. **æ¨¡å‹æ¨ç†ä¼˜åŒ–** - åŠ é€Ÿé¢„æµ‹è¿‡ç¨‹
4. **è®­ç»ƒæ•ˆç‡ä¼˜åŒ–** - æå‡è®­ç»ƒé€Ÿåº¦å’Œç¨³å®šæ€§
5. **ä»£ç ç»“æ„ä¼˜åŒ–** - æ”¹å–„ä»£ç å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§

## ğŸš€ ä¸»è¦ä¼˜åŒ–ç‚¹

### 1. å†…å­˜ç®¡ç†ä¼˜åŒ–

#### é—®é¢˜åˆ†æ
- GPUå†…å­˜æœªåŠæ—¶é‡Šæ”¾
- å¤§æ‰¹é‡å¤„ç†æ—¶å†…å­˜æº¢å‡º
- ç¼ºä¹å†…å­˜ç›‘æ§å’Œè‡ªåŠ¨æ¸…ç†æœºåˆ¶

#### ä¼˜åŒ–æ–¹æ¡ˆ
- å®ç°æ™ºèƒ½å†…å­˜ç®¡ç†å™¨
- æ·»åŠ è‡ªåŠ¨å†…å­˜æ¸…ç†æœºåˆ¶
- ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°åŠ¨æ€è°ƒæ•´

### 2. æ•°æ®åŠ è½½ä¼˜åŒ–

#### é—®é¢˜åˆ†æ
- æ•°æ®åŠ è½½å™¨é…ç½®ä¸å¤Ÿä¼˜åŒ–
- ç¼ºä¹æ•°æ®é¢„å–å’Œç¼“å­˜æœºåˆ¶
- I/Oç“¶é¢ˆå½±å“è®­ç»ƒé€Ÿåº¦

#### ä¼˜åŒ–æ–¹æ¡ˆ
- ä¼˜åŒ–DataLoaderå‚æ•°é…ç½®
- å®ç°æ™ºèƒ½ç¼“å­˜ç­–ç•¥
- æ·»åŠ æ•°æ®é¢„å¤„ç†ç®¡é“ä¼˜åŒ–

### 3. æ¨¡å‹æ¨ç†ä¼˜åŒ–

#### é—®é¢˜åˆ†æ
- å•å¼ å›¾ç‰‡æ¨ç†æ•ˆç‡ä½
- ç¼ºä¹æ¨¡å‹é‡åŒ–å’Œä¼˜åŒ–
- é‡å¤çš„æ¨¡å‹åŠ è½½å’Œåˆå§‹åŒ–

#### ä¼˜åŒ–æ–¹æ¡ˆ
- å®ç°æ‰¹é‡æ¨ç†ä¼˜åŒ–
- æ·»åŠ æ¨¡å‹ç¼“å­˜æœºåˆ¶
- æ”¯æŒæ··åˆç²¾åº¦æ¨ç†

### 4. è®­ç»ƒè¿‡ç¨‹ä¼˜åŒ–

#### é—®é¢˜åˆ†æ
- è®­ç»ƒè¿‡ç¨‹ä¸­å†…å­˜ä½¿ç”¨ä¸ç¨³å®š
- ç¼ºä¹åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´
- éªŒè¯è¿‡ç¨‹æ•ˆç‡ä½ä¸‹

#### ä¼˜åŒ–æ–¹æ¡ˆ
- å®ç°æ¸è¿›å¼è®­ç»ƒç­–ç•¥
- ä¼˜åŒ–éªŒè¯æµç¨‹
- æ·»åŠ è®­ç»ƒç›‘æ§å’Œè‡ªåŠ¨è°ƒä¼˜

## ğŸ“Š å…·ä½“å®ç°

### å†…å­˜ç®¡ç†å™¨å¢å¼º

```python
class EnhancedMemoryManager:
    """å¢å¼ºçš„å†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, gpu_memory_threshold=0.8):
        self.gpu_memory_threshold = gpu_memory_threshold
        self.cleanup_callbacks = []
    
    def auto_cleanup(self):
        """è‡ªåŠ¨å†…å­˜æ¸…ç†"""
        if torch.cuda.is_available():
            current_memory = torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory
            if current_memory > self.gpu_memory_threshold:
                self.aggressive_cleanup()
    
    def aggressive_cleanup(self):
        """æ¿€è¿›çš„å†…å­˜æ¸…ç†"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
```

### æ™ºèƒ½æ‰¹å¤„ç†ä¼˜åŒ–

```python
class AdaptiveBatchProcessor:
    """è‡ªé€‚åº”æ‰¹å¤„ç†å™¨"""
    
    def __init__(self, initial_batch_size=8, min_batch_size=1, max_batch_size=32):
        self.current_batch_size = initial_batch_size
        self.min_batch_size = min_batch_size
        self.max_batch_size = max_batch_size
        self.memory_manager = EnhancedMemoryManager()
    
    def adjust_batch_size(self, success_rate, memory_usage):
        """æ ¹æ®æˆåŠŸç‡å’Œå†…å­˜ä½¿ç”¨æƒ…å†µè°ƒæ•´æ‰¹å¤„ç†å¤§å°"""
        if memory_usage > 0.9:  # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜
            self.current_batch_size = max(self.min_batch_size, self.current_batch_size // 2)
        elif success_rate > 0.95 and memory_usage < 0.7:  # æˆåŠŸç‡é«˜ä¸”å†…å­˜å……è¶³
            self.current_batch_size = min(self.max_batch_size, self.current_batch_size * 2)
```

### æ•°æ®åŠ è½½å™¨ä¼˜åŒ–

```python
def create_optimized_dataloader(dataset, batch_size, device, num_workers=None):
    """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
    if num_workers is None:
        num_workers = min(8, os.cpu_count())
    
    # æ ¹æ®è®¾å¤‡ç±»å‹ä¼˜åŒ–å‚æ•°
    pin_memory = device.type == 'cuda'
    persistent_workers = num_workers > 0
    prefetch_factor = 4 if num_workers > 0 else 2
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor,
        drop_last=True  # é¿å…æœ€åä¸€ä¸ªä¸å®Œæ•´çš„batch
    )
```

### æ¨¡å‹æ¨ç†ä¼˜åŒ–

```python
class OptimizedPredictor:
    """ä¼˜åŒ–çš„é¢„æµ‹å™¨"""
    
    def __init__(self, model_path, config_path, device='auto'):
        self.device = self._select_device(device)
        self.model = self._load_optimized_model(model_path, config_path)
        self.memory_manager = EnhancedMemoryManager()
        self.batch_processor = AdaptiveBatchProcessor()
    
    @torch.inference_mode()  # æ›´é«˜æ•ˆçš„æ¨ç†æ¨¡å¼
    def predict_batch(self, images):
        """æ‰¹é‡é¢„æµ‹ä¼˜åŒ–"""
        try:
            # è‡ªåŠ¨è°ƒæ•´æ‰¹å¤„ç†å¤§å°
            batch_size = self.batch_processor.current_batch_size
            results = []
            
            for i in range(0, len(images), batch_size):
                batch = images[i:i+batch_size]
                batch_tensor = torch.stack(batch).to(self.device, non_blocking=True)
                
                # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
                with torch.cuda.amp.autocast(enabled=self.device.type == 'cuda'):
                    outputs = self.model(batch_tensor)
                    predictions = torch.sigmoid(outputs)
                
                # ç«‹å³ç§»åŠ¨åˆ°CPUå¹¶æ¸…ç†
                results.extend(predictions.cpu())
                del batch_tensor, outputs, predictions
                
                # å®šæœŸæ¸…ç†å†…å­˜
                if i % (batch_size * 4) == 0:
                    self.memory_manager.auto_cleanup()
            
            return results
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                # å†…å­˜ä¸è¶³æ—¶è‡ªåŠ¨é™ä½æ‰¹å¤„ç†å¤§å°
                self.batch_processor.current_batch_size = max(1, self.batch_processor.current_batch_size // 2)
                self.memory_manager.aggressive_cleanup()
                return self.predict_batch(images)  # é€’å½’é‡è¯•
            raise e
```

## ğŸ”§ é…ç½®ä¼˜åŒ–å»ºè®®

### è®­ç»ƒé…ç½®ä¼˜åŒ–

```yaml
# ä¼˜åŒ–åçš„è®­ç»ƒé…ç½®
TRAIN:
  BATCH_SIZE: 8  # æ ¹æ®GPUå†…å­˜åŠ¨æ€è°ƒæ•´
  GRADIENT_ACCUMULATION_STEPS: 2  # æ¢¯åº¦ç´¯ç§¯
  MIXED_PRECISION: true  # æ··åˆç²¾åº¦è®­ç»ƒ
  GRADIENT_CLIP: 1.0  # æ¢¯åº¦è£å‰ª

DATA:
  NUM_WORKERS: 8  # æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹
  PREFETCH_FACTOR: 4  # é¢„å–å› å­
  PIN_MEMORY: true  # å›ºå®šå†…å­˜
  PERSISTENT_WORKERS: true  # æŒä¹…åŒ–å·¥ä½œè¿›ç¨‹
  CACHE_IMAGES: true  # å›¾åƒç¼“å­˜

OPTIMIZER:
  NAME: "AdamW"  # æ›´ç¨³å®šçš„ä¼˜åŒ–å™¨
  LR: 0.001
  WEIGHT_DECAY: 0.01
  BETAS: [0.9, 0.999]
  EPS: 1e-8
```

### é¢„æµ‹é…ç½®ä¼˜åŒ–

```yaml
PREDICT:
  BATCH_SIZE: 16  # é¢„æµ‹æ—¶å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹å¤„ç†
  AUTO_BATCH_SIZE: true  # è‡ªåŠ¨è°ƒæ•´æ‰¹å¤„ç†å¤§å°
  MAX_BATCH_SIZE: 32
  MIN_BATCH_SIZE: 1
  MEMORY_THRESHOLD: 0.8  # å†…å­˜ä½¿ç”¨é˜ˆå€¼
  USE_MIXED_PRECISION: true  # æ··åˆç²¾åº¦æ¨ç†
  NON_BLOCKING: true  # éé˜»å¡æ•°æ®ä¼ è¾“
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### å®æ—¶æ€§èƒ½ç›‘æ§

```python
class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            'gpu_memory': [],
            'cpu_usage': [],
            'processing_time': [],
            'batch_sizes': []
        }
    
    def log_metrics(self, gpu_memory, cpu_usage, processing_time, batch_size):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        self.metrics['gpu_memory'].append(gpu_memory)
        self.metrics['cpu_usage'].append(cpu_usage)
        self.metrics['processing_time'].append(processing_time)
        self.metrics['batch_sizes'].append(batch_size)
    
    def get_performance_summary(self):
        """è·å–æ€§èƒ½æ‘˜è¦"""
        return {
            'avg_gpu_memory': np.mean(self.metrics['gpu_memory']),
            'max_gpu_memory': np.max(self.metrics['gpu_memory']),
            'avg_processing_time': np.mean(self.metrics['processing_time']),
            'optimal_batch_size': self._calculate_optimal_batch_size()
        }
```

## ğŸ¯ å®æ–½å»ºè®®

### é˜¶æ®µ1ï¼šåŸºç¡€ä¼˜åŒ–ï¼ˆç«‹å³å®æ–½ï¼‰
1. æ·»åŠ å†…å­˜ç®¡ç†å™¨åˆ°ç°æœ‰ä»£ç 
2. ä¼˜åŒ–DataLoaderé…ç½®
3. å®ç°è‡ªåŠ¨å†…å­˜æ¸…ç†

### é˜¶æ®µ2ï¼šè¿›é˜¶ä¼˜åŒ–ï¼ˆ1-2å‘¨å†…ï¼‰
1. å®ç°è‡ªé€‚åº”æ‰¹å¤„ç†
2. æ·»åŠ æ··åˆç²¾åº¦æ”¯æŒ
3. ä¼˜åŒ–æ¨¡å‹æ¨ç†æµç¨‹

### é˜¶æ®µ3ï¼šé«˜çº§ä¼˜åŒ–ï¼ˆé•¿æœŸï¼‰
1. å®ç°æ¨¡å‹é‡åŒ–
2. æ·»åŠ åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
3. å®ç°ç«¯åˆ°ç«¯æ€§èƒ½ä¼˜åŒ–

## ğŸ“Š é¢„æœŸæ•ˆæœ

- **å†…å­˜ä½¿ç”¨å‡å°‘**: 30-50%
- **è®­ç»ƒé€Ÿåº¦æå‡**: 20-40%
- **æ¨ç†é€Ÿåº¦æå‡**: 40-60%
- **ç¨³å®šæ€§æ”¹å–„**: æ˜¾è‘—å‡å°‘OOMé”™è¯¯
- **èµ„æºåˆ©ç”¨ç‡**: æå‡20-30%

## ğŸ” ç›‘æ§å’Œè°ƒä¼˜

### å…³é”®æŒ‡æ ‡ç›‘æ§
- GPUå†…å­˜ä½¿ç”¨ç‡
- æ‰¹å¤„ç†æˆåŠŸç‡
- å¹³å‡å¤„ç†æ—¶é—´
- æ¨¡å‹ç²¾åº¦å˜åŒ–

### è‡ªåŠ¨è°ƒä¼˜ç­–ç•¥
- æ ¹æ®ç¡¬ä»¶é…ç½®è‡ªåŠ¨è°ƒæ•´å‚æ•°
- åŸºäºå†å²æ€§èƒ½æ•°æ®ä¼˜åŒ–é…ç½®
- å®æ—¶ç›‘æ§å’ŒåŠ¨æ€è°ƒæ•´

---

*æ­¤ä¼˜åŒ–æ–¹æ¡ˆåŸºäºå½“å‰ä»£ç åˆ†æï¼Œå»ºè®®åˆ†é˜¶æ®µå®æ–½å¹¶æŒç»­ç›‘æ§æ•ˆæœã€‚*